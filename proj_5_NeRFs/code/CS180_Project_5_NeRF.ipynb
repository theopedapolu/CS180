{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL0BTQ8MZ4i9"
      },
      "source": [
        "# CS 180 Project 5 - Neural Radiance Fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWXAQRr4Z4lc"
      },
      "source": [
        "# Part 1 - Fit a Neural Field to a 2D image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9raYEW8oaC7u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import skimage.io as skio\n",
        "import skimage as sk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzU-LqB8-7se"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adDU6W6daC0B"
      },
      "outputs": [],
      "source": [
        "# Build Model\n",
        "class NF_MLP(nn.Module):\n",
        "  def __init__(self, channel_size, L):\n",
        "    super(NF_MLP, self).__init__()\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(4*L+2, channel_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(channel_size,channel_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(channel_size,channel_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(channel_size,3),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.L = L\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.get_PE(x)\n",
        "    out = self.mlp(x)\n",
        "    return out\n",
        "\n",
        "  def get_PE(self, x):\n",
        "    x_dims = torch.hstack([torch.sin(((2**i)*np.pi)*x) for i in range(self.L)])\n",
        "    y_dims = torch.hstack([torch.cos(((2**i)*np.pi)*x) for i in range(self.L)])\n",
        "    x_PE = torch.hstack((x,x_dims, y_dims))\n",
        "    return x_PE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWBkM2oHez_m"
      },
      "outputs": [],
      "source": [
        "# Create Custom Dataset\n",
        "class NeRF_Dataset(Dataset):\n",
        "  def __init__(self,image,N, numIt):\n",
        "    self.N = N\n",
        "    self.numIt = numIt\n",
        "    self.image = image\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.numIt\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    N,M,_ = self.image.shape\n",
        "    random_indices = torch.randint(0, N*M, (self.N,))\n",
        "    row_indices = random_indices // M\n",
        "    col_indices = random_indices % M\n",
        "    pixel_coords = torch.stack((row_indices/N, col_indices/M), dim=1)\n",
        "    pixel_vals = self.image[row_indices, col_indices,:]\n",
        "    return pixel_coords, pixel_vals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oI2Nv8_PuZE"
      },
      "outputs": [],
      "source": [
        "# Function to run model on image\n",
        "def getImage(model, s):\n",
        "  model.eval()\n",
        "  N,M,_ = s\n",
        "  row_coords, col_coords = torch.meshgrid(torch.arange(N), torch.arange(M))\n",
        "  pixel_coords = torch.stack((row_coords/N, col_coords/M), dim=2)\n",
        "  img = model(pixel_coords.reshape(N*M,2).to(device)).reshape(N,M,3).cpu().detach().numpy()\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GinBiaS6e0E-"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Funtion to train model to reconstruct image\n",
        "def train(model, train_loader,s):\n",
        "  losses = []\n",
        "  ims = []\n",
        "\n",
        "  model.train()\n",
        "  PSNR_loss = lambda input, target: 10*torch.log10(nn.functional.mse_loss(input.float(), target.float()))\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "  steps = [1,20,100,500,1000,2000]\n",
        "  for i, (inputs, labels) in enumerate(tqdm(train_loader)):\n",
        "      if i+1 in steps:\n",
        "        im = getImage(model,s)\n",
        "        ims.append(im)\n",
        "      inputs = inputs[0].to(device)\n",
        "      labels = labels[0].to(device)\n",
        "      outputs = model(inputs)\n",
        "      loss = PSNR_loss(outputs, labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return losses, ims"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf7RTNvaPrNZ"
      },
      "source": [
        "## Run NF_MLP model on images"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fox Image Results"
      ],
      "metadata": {
        "id": "AV03Rw9L_9Ur"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEcLMhvRwvjt"
      },
      "outputs": [],
      "source": [
        "fox_image = torch.tensor(sk.img_as_float(skio.imread(\"fox.jpg\")))\n",
        "foxDataset = NeRF_Dataset(fox_image,10000,2000)\n",
        "fox_dataloader = DataLoader(foxDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaVDl4QnhMiv"
      },
      "outputs": [],
      "source": [
        "model = NF_MLP().to(device)\n",
        "fox_losses, fox_ims = train(model, fox_dataloader,fox_image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjIaFA5m5MGD"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1,2001),-np.array(fox_losses),'-bo',markersize=0.5)\n",
        "plt.xlabel('Number of Iterations')\n",
        "plt.ylabel('PSNR')\n",
        "plt.title('Fox Image Training PSNR')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEmuuTSC7iqh"
      },
      "outputs": [],
      "source": [
        "fig,axes = plt.subplots(1,len(fox_ims),figsize=(30,30))\n",
        "plt.subplots_adjust(wspace=0.05)\n",
        "for i in range(len(fox_ims)):\n",
        "  axes[i].imshow(fox_ims[i])\n",
        "  axes[i].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fox Image Results - Increase Channel Size"
      ],
      "metadata": {
        "id": "efAZFie4ANF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = NF_MLP(420,10).to(device)\n",
        "fox_losses2, fox_ims2 = train(model2, fox_dataloader,fox_image.shape)"
      ],
      "metadata": {
        "id": "zmCdXLo8AMlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1,2001),-np.array(fox_losses2),'-bo',markersize=0.5)\n",
        "plt.xlabel('Number of Iterations')\n",
        "plt.ylabel('PSNR')\n",
        "plt.title('Fox Image Training PSNR with Channel Size = 420')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a2Ze6a24Dh7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,axes = plt.subplots(1,len(fox_ims2),figsize=(30,30))\n",
        "plt.subplots_adjust(wspace=0.05)\n",
        "for i in range(len(fox_ims2)):\n",
        "  axes[i].imshow(fox_ims2[i])\n",
        "  axes[i].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fiQuBxfZE6pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fox Image Results - Decrease Encoding Size"
      ],
      "metadata": {
        "id": "RhZPyggcGRgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = NF_MLP(256,3).to(device)\n",
        "fox_losses3, fox_ims3 = train(model3, fox_dataloader,fox_image.shape)"
      ],
      "metadata": {
        "id": "cftZuLWEGY2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1,2001),-np.array(fox_losses3),'-bo',markersize=0.5)\n",
        "plt.xlabel('Number of Iterations')\n",
        "plt.ylabel('PSNR')\n",
        "plt.title('Fox Image Training PSNR with L = 3')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cK2V9xzgGY8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,axes = plt.subplots(1,len(fox_ims3),figsize=(30,30))\n",
        "plt.subplots_adjust(wspace=0.05)\n",
        "for i in range(len(fox_ims3)):\n",
        "  axes[i].imshow(fox_ims3[i])\n",
        "  axes[i].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4xx-gwrwGZA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results for El Capitan image"
      ],
      "metadata": {
        "id": "Rk3K4UDk-jKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "capitan_image = torch.tensor(sk.img_as_float(skio.imread(\"capitan.jpeg\")))\n",
        "capitanDataset = NeRF_Dataset(capitan_image,30000,2000)\n",
        "capitan_dataloader = DataLoader(capitanDataset)"
      ],
      "metadata": {
        "id": "wNMDB-Fk-iim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "capitanModel = NF_MLP(300,15).to(device)\n",
        "capitan_losses, capitan_ims = train(capitanModel, capitan_dataloader,capitan_image.shape)"
      ],
      "metadata": {
        "id": "bDmhfyhE_ZHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1,2001),-np.array(capitan_losses),'-bo',markersize=0.5)\n",
        "plt.xlabel('Number of Iterations')\n",
        "plt.ylabel('PSNR')\n",
        "plt.title('El Capitan Image Training PSNR')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JJRntae3_ZM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,axes = plt.subplots(1,len(capitan_ims),figsize=(30,30))\n",
        "plt.subplots_adjust(wspace=0.05)\n",
        "for i in range(len(capitan_ims)):\n",
        "  axes[i].imshow(capitan_ims[i])\n",
        "  axes[i].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zITThr6-_0SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNP4U8pdHgNq"
      },
      "source": [
        "## Part 2: Fit a Neural Radiance Field from Multi-view images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vg3KQVFJaW2E"
      },
      "outputs": [],
      "source": [
        "data = np.load(f\"lego_200x200.npz\")\n",
        "\n",
        "# Training images: [100, 200, 200, 3]\n",
        "images_train = data[\"images_train\"] / 255.0\n",
        "\n",
        "# Cameras for the training images\n",
        "# (camera-to-world transformation matrix): [100, 4, 4]\n",
        "c2ws_train = data[\"c2ws_train\"]\n",
        "\n",
        "# Validation images:\n",
        "images_val = data[\"images_val\"] / 255.0\n",
        "\n",
        "# Cameras for the validation images: [10, 4, 4]\n",
        "# (camera-to-world transformation matrix): [10, 200, 200, 3]\n",
        "c2ws_val = data[\"c2ws_val\"]\n",
        "\n",
        "# Test cameras for novel-view video rendering:\n",
        "# (camera-to-world transformation matrix): [60, 4, 4]\n",
        "c2ws_test = data[\"c2ws_test\"]\n",
        "\n",
        "# Camera focal length\n",
        "focal = data[\"focal\"]  # float\n",
        "\n",
        "# Intrinsic matrix\n",
        "sigma = images_train.shape[1]/2\n",
        "K = np.array([[focal,0,sigma],[0,focal,sigma],[0,0,1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjJb_NUlHrR4"
      },
      "source": [
        "### Part 2.1: Create rays from cameras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GofFDO34Hfm_"
      },
      "outputs": [],
      "source": [
        "def transform(c2w,x_c):\n",
        "  coords_set = np.hstack([x_c,np.ones((x_c.shape[0],1))]).T\n",
        "  x_w = c2w @ coords_set\n",
        "  x_w = x_w[:3] / x_w[3]\n",
        "  return x_w.T\n",
        "\n",
        "def pixel_to_camera(K, uv, s):\n",
        "  uv = np.hstack([uv,np.ones((uv.shape[0],1))]).T\n",
        "  K_inv = np.linalg.inv(K)\n",
        "  return (K_inv * s @ uv).T\n",
        "\n",
        "def pixel_to_ray(K, c2w, uv):\n",
        "  uv = uv + 0.5\n",
        "  r_o = transform(c2w, np.zeros((uv.shape[0],3)))\n",
        "  X_c = pixel_to_camera(K,uv, 1)\n",
        "  X_w = transform(c2w, X_c)\n",
        "  norm = np.linalg.norm(X_w-r_o, axis=1)\n",
        "  r_d = (X_w - r_o) / norm.reshape((norm.shape[0],1))\n",
        "  return r_o, r_d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMLLhNZGInMk"
      },
      "source": [
        "### Parts 2.2 & 2.3: Sampling and putting Dataloading together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t11SfCvLkgaJ"
      },
      "outputs": [],
      "source": [
        "def sample_along_rays(rays_o, rays_d, perturb):\n",
        "    near = 2.0\n",
        "    far = 6.0\n",
        "    n_samples = 64\n",
        "    t = np.linspace(near, far, n_samples)\n",
        "    if perturb:\n",
        "      t = t + np.random.rand(n_samples) * 0.1\n",
        "\n",
        "    X = np.array([rays_o + rays_d*z for z in t])\n",
        "    return X.swapaxes(0,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkttCiR0Iq5M"
      },
      "outputs": [],
      "source": [
        "class RaysData(Dataset):\n",
        "  def __init__(self,images, K, c2ws, M, numIt,perturb):\n",
        "    self.images = images\n",
        "    self.K = K\n",
        "    self.c2ws = c2ws\n",
        "    self.perturb = perturb\n",
        "    self.M = M\n",
        "    self.numIt = numIt\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.numIt\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    rays_o, rays_d, pixels = self.sample_rays(10000)\n",
        "    points = sample_along_rays(rays_o, rays_d,self.perturb)\n",
        "    return rays_d, points, pixels\n",
        "\n",
        "  def sample_rays(self, N):\n",
        "    dim = self.images[0].shape[1]\n",
        "    M = self.M\n",
        "    sample_idx = np.random.choice(np.arange(self.images.shape[0]),M)\n",
        "    image_sample = self.images[sample_idx]\n",
        "    c2w_sample = self.c2ws[sample_idx]\n",
        "    sample_pixels = np.random.choice(np.arange(dim),(M,N//M,2))\n",
        "\n",
        "    rays_o = np.zeros(((N//M)*M,3))\n",
        "    rays_d = np.zeros(((N//M)*M,3))\n",
        "    pixels = np.zeros(((N//M)*M,3))\n",
        "    for i in range(M):\n",
        "      rays_o[(N//M)*i:(N//M)*(i+1)], rays_d[(N//M)*i:(N//M)*(i+1)] = pixel_to_ray(self.K, c2w_sample[i],sample_pixels[i])\n",
        "      pixels[(N//M)*i:(N//M)*(i+1)] = image_sample[i,sample_pixels[i][:,1],sample_pixels[i][:,0]]\n",
        "    return rays_o, rays_d, pixels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP_vcc-6O0s0"
      },
      "outputs": [],
      "source": [
        "%pip install viser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCNcUWVOaNrN"
      },
      "outputs": [],
      "source": [
        "import viser, time  # pip install viser\n",
        "import numpy as np\n",
        "\n",
        "# --- You Need to Implement These ------\n",
        "dataset = RaysData(images_train, K, c2ws_train)\n",
        "rays_o, rays_d, pixels = dataset.sample_rays(100)\n",
        "points = dataset.sample_along_rays(rays_o, rays_d, perturb=True)\n",
        "H, W = images_train.shape[1:3]\n",
        "# ---------------------------------------\n",
        "\n",
        "server = viser.ViserServer(share=True)\n",
        "for i, (image, c2w) in enumerate(zip(images_train, c2ws_train)):\n",
        "    server.add_camera_frustum(\n",
        "        f\"/cameras/{i}\",\n",
        "        fov=2 * np.arctan2(H / 2, K[0, 0]),\n",
        "        aspect=W / H,\n",
        "        scale=0.15,\n",
        "        wxyz=viser.transforms.SO3.from_matrix(c2w[:3, :3]).wxyz,\n",
        "        position=c2w[:3, 3],\n",
        "        image=image\n",
        "    )\n",
        "for i, (o, d) in enumerate(zip(rays_o, rays_d)):\n",
        "    server.add_spline_catmull_rom(\n",
        "        f\"/rays/{i}\", positions=np.stack((o, o + d * 6.0)),\n",
        "    )\n",
        "server.add_point_cloud(\n",
        "    f\"/samples\",\n",
        "    colors=np.zeros_like(points).reshape(-1, 3),\n",
        "    points=points.reshape(-1, 3),\n",
        "    point_size=0.02,\n",
        ")\n",
        "time.sleep(1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXQMkR9KIq_J"
      },
      "source": [
        "### Parts 2.4 & 2.5: Neural Radiance Field and Volume Rendering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKRYMsobIvzV"
      },
      "outputs": [],
      "source": [
        "class NeRF(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeRF, self).__init__()\n",
        "    self.centerMLP1 = nn.Sequential(\n",
        "        nn.Linear(63, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256,256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256,256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256,256),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    self.centerMLP2 = nn.Sequential(\n",
        "        nn.Linear(319,256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256,256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256,256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256,256)\n",
        "    )\n",
        "\n",
        "    self.densityMLP = nn.Sequential(\n",
        "        nn.Linear(256,1),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    self.rgbMLP1 = nn.Linear(256,256)\n",
        "    self.rgbMLP2 = nn.Sequential(\n",
        "        nn.Linear(283,128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128,3),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self,x,r_d):\n",
        "    B,N,_ = x.size()\n",
        "    x = self.get_PE(x,10)\n",
        "    r_d = self.get_PE(r_d,4)\n",
        "    # Feed forward\n",
        "    x = torch.cat([self.centerMLP1(x),x],dim =-1)\n",
        "    x = self.centerMLP2(x)\n",
        "\n",
        "    # Go through density branch\n",
        "    density = self.densityMLP(x)\n",
        "\n",
        "    # Go through RGB branch\n",
        "    rgb = self.rgbMLP1(x)\n",
        "    r_d = r_d.unsqueeze(1).repeat(1,N,1)\n",
        "    rgb = torch.cat([rgb,r_d], dim=-1)\n",
        "    rgb = self.rgbMLP2(rgb)\n",
        "\n",
        "    # Volume Rendering\n",
        "    step_size = (6.0 - 2.0) / 64\n",
        "    return self.volrend(density,rgb,step_size)\n",
        "\n",
        "  def get_PE(self, x, L):\n",
        "    sin_dims = torch.cat([torch.sin(((2**i)*np.pi)*x) for i in range(L)],dim=-1)\n",
        "    cos_dims = torch.cat([torch.cos(((2**i)*np.pi)*x) for i in range(L)],dim=-1)\n",
        "    x_PE = torch.cat([x,sin_dims, cos_dims],dim=-1)\n",
        "    return x_PE\n",
        "\n",
        "  def volrend(self, sigmas, rgbs, step_size):\n",
        "    B = sigmas.size(0)\n",
        "    sigmas = torch.cat([torch.zeros(B,1,1).to(device),sigmas],dim=1)\n",
        "    T_i = torch.exp(-torch.cumsum(sigmas[:,:-1,:], dim=1)*step_size)\n",
        "    exp_term = 1-torch.exp(-sigmas[:,1:,:]*step_size)\n",
        "    #prod = T_i * exp_term * torch.linspace(1,0,64).unsqueeze(0).unsqueeze(2).repeat(B,1,1).to(device) #depth rendering\n",
        "    prod = T_i * exp_term * rgbs\n",
        "    rend = torch.sum(prod, dim=1)\n",
        "    color = torch.Tensor([0,0.667,1]).to(device)\n",
        "    rend = rend + (1-torch.sum(T_i*exp_term,dim=1))*color # bg color\n",
        "    return rend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75hVo9JOe-7e"
      },
      "source": [
        "### Test Volume Rendering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyFHUTHgD7RG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(42)\n",
        "sigmas = torch.rand((10, 64, 1))\n",
        "rgbs = torch.rand((10, 64, 3))\n",
        "step_size = (6.0 - 2.0) / 64\n",
        "rendered_colors = volrend(sigmas, rgbs, step_size)\n",
        "\n",
        "correct = torch.tensor([\n",
        "    [0.5006, 0.3728, 0.4728],\n",
        "    [0.4322, 0.3559, 0.4134],\n",
        "    [0.4027, 0.4394, 0.4610],\n",
        "    [0.4514, 0.3829, 0.4196],\n",
        "    [0.4002, 0.4599, 0.4103],\n",
        "    [0.4471, 0.4044, 0.4069],\n",
        "    [0.4285, 0.4072, 0.3777],\n",
        "    [0.4152, 0.4190, 0.4361],\n",
        "    [0.4051, 0.3651, 0.3969],\n",
        "    [0.3253, 0.3587, 0.4215]\n",
        "  ])\n",
        "print(rendered_colors)\n",
        "assert torch.allclose(rendered_colors, correct, rtol=1e-4, atol=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n11TbFi9TD_Q"
      },
      "source": [
        "### Train NeRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hq1QowW6TOTB"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def trainNeRF(model, train_loader):\n",
        "  model.train()\n",
        "  PSNR_loss = lambda input, target: 10*torch.log10(nn.functional.mse_loss(input, target))\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "  val_losses = []\n",
        "  ims = []\n",
        "  steps = [1,50,100,200,500,1000]\n",
        "  target = torch.Tensor(images_val[0].reshape(200*200,3))\n",
        "\n",
        "  for i, (rays_d, points, pixels) in enumerate(tqdm(train_loader)):\n",
        "      rays_d = rays_d[0].to(device).to(torch.float32)\n",
        "      points = points[0].to(device).to(torch.float32)\n",
        "      pixels = pixels[0].to(device).to(torch.float32)\n",
        "      outputs = model(points, rays_d)\n",
        "      loss = PSNR_loss(outputs, pixels)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      #im = torch.Tensor(renderImage(model,200,200,K,c2ws_val[0]))\n",
        "      # if i+1 in steps:\n",
        "      #   ims.append(im)\n",
        "      # val_losses.append(PSNR_loss(im.reshape(200*200,3),target))\n",
        "      print(loss.item())\n",
        "\n",
        "  return val_losses,ims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm7PRA9bkxR_"
      },
      "outputs": [],
      "source": [
        "lego_dataset = RaysData(images_train,K,c2ws_train,50,5000,True)\n",
        "lego_train_loader = DataLoader(lego_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlNPU9_Zj5AR"
      },
      "outputs": [],
      "source": [
        "model = NeRF().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np1vOyghkmqu"
      },
      "outputs": [],
      "source": [
        "lego_losses, lego_ims = trainNeRF(model, lego_train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1,1001),-np.array(lego_losses),'-bo',markersize=0.5)\n",
        "plt.xlabel('Number of Training Iterations')\n",
        "plt.ylabel('PSNR')\n",
        "plt.title('Lego Image Validation PSNR')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Uv-xfsTIOTfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,axes = plt.subplots(1,len(lego_ims),figsize=(30,30))\n",
        "plt.subplots_adjust(wspace=0.05)\n",
        "for i in range(len(lego_ims)):\n",
        "  axes[i].imshow(lego_ims[i])\n",
        "  axes[i].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3hb4KJgoOig6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv25YzjVLw4H"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'model2.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8wXR4Buh6O3"
      },
      "source": [
        "### Run NeRF Model on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1En5s8KoKUpF"
      },
      "outputs": [],
      "source": [
        "val_dataset = RaysData(images_val,K,c2ws_val, 10, False)\n",
        "val_loader = DataLoader(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOJC-OI_HiqM"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "def testNeRF(model, val_loader):\n",
        "  model.eval()\n",
        "  PSNR_loss = lambda input, target: 10*torch.log10(nn.functional.mse_loss(input.float(), target.float()))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, (rays_d, points, pixels) in enumerate(tqdm(val_loader)):\n",
        "        rays_d = rays_d[0].to(torch.float32).to(device)\n",
        "        points = points[0].to(torch.float32).to(device)\n",
        "        pixels = pixels[0].to(torch.float32).to(device)\n",
        "        outputs = model(points, rays_d)\n",
        "        loss = PSNR_loss(outputs, pixels)\n",
        "        print(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hanKjcTZLj69"
      },
      "outputs": [],
      "source": [
        "model = NeRF().to(device)\n",
        "model.load_state_dict(torch.load(\"model2.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TupRPwQ7SFC"
      },
      "outputs": [],
      "source": [
        "testNeRF(model, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avV-N3PDIwHI"
      },
      "source": [
        "### Render Lego Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqAGW8LDjxws"
      },
      "outputs": [],
      "source": [
        "model = NeRF().to(device)\n",
        "model.load_state_dict(torch.load(\"model.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1V-Cy8nfhNX"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# Function to render a single image\n",
        "def renderImage(model, width, height, K, c2w):\n",
        "  w_step = width // 4\n",
        "  h_step = height // 4\n",
        "  image = np.ones((height,width,3))\n",
        "  for i in range(4):\n",
        "    for j in range(4):\n",
        "      coords = np.array(np.meshgrid(np.arange(i*w_step,(i+1)*w_step),np.arange(j*h_step,(j+1)*h_step)))\n",
        "      coords = coords.reshape(-1,coords.shape[1]*coords.shape[2]).T\n",
        "      rays_o, rays_d = pixel_to_ray(K,c2w,coords)\n",
        "      X = sample_along_rays(rays_o, rays_d, False)\n",
        "      X = torch.Tensor(X).to(device)\n",
        "      rays_d = torch.Tensor(rays_d).to(device)\n",
        "      #imagePatch = model(X,rays_d).cpu().detach().numpy().reshape(height//4,width//4,1) #depth rendering\n",
        "      imagePatch = model(X,rays_d).cpu().detach().numpy().reshape(height//4,width//4,3)\n",
        "      image[j*h_step:(j+1)*h_step,i*w_step:(i+1)*w_step] = imagePatch\n",
        "  return image\n",
        "\n",
        "def renderFrames(model, width, height, K, c2ws):\n",
        "  ims = []\n",
        "  for i in range(len(c2ws)):\n",
        "    image = renderImage(model, width, height, K, c2ws[i])\n",
        "    ims.append(sk.img_as_ubyte(image))\n",
        "  return ims\n",
        "\n",
        "def createVideo(name, images):\n",
        "  video_writer = cv2.VideoWriter(name, cv2.VideoWriter_fourcc(*'mp4v'),10,(images[0].shape[1], images[0].shape[0]))\n",
        "  for im in images:\n",
        "      video_writer.write(im)\n",
        "  video_writer.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLG1uYZchYDb"
      },
      "outputs": [],
      "source": [
        "## Normal video\n",
        "# Create frames\n",
        "lego_render_ims = renderFrames(model,200,200,K,c2ws_test)\n",
        "lego_render_ims = [im[:,:,::-1] for im in lego_render_ims]\n",
        "\n",
        "# Write video\n",
        "createVideo(\"lego_depth.mp4\", lego_render_ims)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Depth video\n",
        "# Create frames\n",
        "lego_depth_render_ims = renderFrames(model,200,200,K,c2ws_test)\n",
        "lego_depth_render_ims = [im[:,:,::-1] for im in lego_depth_render_ims]\n",
        "\n",
        "# Write video\n",
        "createVideo(\"lego_depth.mp4\", lego_depth_render_ims)"
      ],
      "metadata": {
        "id": "MhUkKLhDcG6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## BG Color video\n",
        "# Create frames\n",
        "lego_color_render_ims = renderFrames(model,200,200,K,c2ws_test)\n",
        "lego_color_render_ims = [im[:,:,::-1] for im in lego_color_render_ims]\n",
        "\n",
        "# Write video\n",
        "createVideo(\"lego_color.mp4\", lego_color_render_ims)"
      ],
      "metadata": {
        "id": "ZWaL3RyXctmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im = renderImage(model,200,200,K,c2ws_test[1])"
      ],
      "metadata": {
        "id": "nbxm1CaFY_1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(im)"
      ],
      "metadata": {
        "id": "U8C9pI1dsbtP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}